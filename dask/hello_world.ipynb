{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.datasets\n",
    "\n",
    "ddf = dask.datasets.timeseries()\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = ddf[ddf.y > 0]\n",
    "ddf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf3 = ddf2.groupby(\"name\").x.std()\n",
    "ddf3.visualize(\"./view/ddf3\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf3.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf3.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "engine = 'clickhouse://default:@10.9.0.32:8123/spark_test'\n",
    "# ddf = dd.read_sql_table(\"(SELECT id, arrayStringConcat(feature, ',') as feature FROM face_features)\", engine, index_col=None, npartitions=10)\n",
    "ddf = dd.read_sql_table(\"random_array\", engine, index_col=\"id\", npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of Dask DataFrame Structure:\n",
       "                   col1     col2     col3     col4     col5     col6     col7     col8     col9    col10    col11    col12    col13    col14    col15    col16    col17    col18    col19    col20\n",
       "npartitions=10                                                                                                                                                                                    \n",
       "0.0             float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64\n",
       "99.9                ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "...                 ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "899.1               ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "999.0               ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "Dask Name: from-delayed, 1 expression\n",
       "Expr=FromGraph(8103fb2)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 18:46:55,176 - distributed.scheduler - ERROR - Error during deserialization of the task graph. This frequently\n",
      "occurs if the Scheduler and Client have different environments.\n",
      "For more information, see\n",
      "https://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n",
      "\n",
      "2024-07-18 18:46:55,504 - distributed.protocol.pickle - ERROR - Failed to serialize Error during deserialization of the task graph. This frequently\n",
      "occurs if the Scheduler and Client have different environments.\n",
      "For more information, see\n",
      "https://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n",
      ".\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/scheduler.py\", line 4741, in update_graph\n",
      "    graph = deserialize(graph_header, graph_frames).data\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/serialize.py\", line 449, in deserialize\n",
      "    return loads(header, frames)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/serialize.py\", line 111, in pickle_loads\n",
      "    return pickle.loads(pik, buffers=buffers)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 94, in loads\n",
      "    return pickle.loads(x, buffers=buffers)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/sqlalchemy/sql/base.py\", line 2112, in __setstate__\n",
      "    self.__init__(parent)  # type: ignore\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/sqlalchemy/sql/base.py\", line 2102, in __init__\n",
      "    object.__setattr__(self, \"_colset\", collection._colset)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/sqlalchemy/sql/base.py\", line 1617, in __getattr__\n",
      "    return self._index[key][1]\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/sqlalchemy/sql/base.py\", line 1617, in __getattr__\n",
      "    return self._index[key][1]\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/sqlalchemy/sql/base.py\", line 1617, in __getattr__\n",
      "    return self._index[key][1]\n",
      "  [Previous line repeated 2973 more times]\n",
      "RecursionError: maximum recursion depth exceeded\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/scheduler.py\", line 4750, in update_graph\n",
      "    raise RuntimeError(textwrap.dedent(msg)) from e\n",
      "RuntimeError: Error during deserialization of the task graph. This frequently\n",
      "occurs if the Scheduler and Client have different environments.\n",
      "For more information, see\n",
      "https://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\n",
      "    result = pickle.dumps(x, **dump_kwargs)\n",
      "RecursionError: maximum recursion depth exceeded while pickling an object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\n",
      "    pickler.dump(x)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 24, in reducer_override\n",
      "    if _always_use_pickle_for(obj):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 35, in _always_use_pickle_for\n",
      "    mod, _, _ = x.__class__.__module__.partition(\".\")\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
      "    return super().dump(obj)\n",
      "RecursionError: maximum recursion depth exceeded while pickling an object\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\n",
      "    result = cloudpickle.dumps(x, **dump_kwargs)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/cloudpickle/cloudpickle.py\", line 1249, in dump\n",
      "    raise pickle.PicklingError(msg) from e\n",
      "_pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "2024-07-18 18:47:41,727 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8c3519de87a6bfce30fdb93227660f1 initialized by task ('shuffle-transfer-a8c3519de87a6bfce30fdb93227660f1', 15) executed on worker tcp://127.0.0.1:33558\n",
      "2024-07-18 18:47:42,230 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a8c3519de87a6bfce30fdb93227660f1 deactivated due to stimulus 'task-finished-1721299662.2266872'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "RuntimeError('Error during deserialization of the task graph. This frequently\\noccurs if the Scheduler and Client have different environments.\\nFor more information, see\\nhttps://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ddf\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m----> 2\u001b[0m \u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dask/lib/python3.9/site-packages/dask_expr/_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dask/lib/python3.9/site-packages/dask/base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/dask/lib/python3.9/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/dask/lib/python3.9/site-packages/distributed/client.py:2310\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     exception \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m   2309\u001b[0m     traceback \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtraceback\n\u001b[0;32m-> 2310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2312\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[0;31mException\u001b[0m: RuntimeError('Error during deserialization of the task graph. This frequently\\noccurs if the Scheduler and Client have different environments.\\nFor more information, see\\nhttps://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\\n')"
     ]
    }
   ],
   "source": [
    "ddf.columns\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuaikun/miniconda3/envs/dask/lib/python3.9/site-packages/dask_expr/_collection.py:5968: UserWarning: dask_expr does not support the DataFrameIOFunction protocol for column projection. To enable column projection, please ensure that the signature of `func` includes a `columns=` keyword argument instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Bob         0.576393\n",
      "Dan         0.576849\n",
      "Edith       0.576161\n",
      "George      0.578666\n",
      "Hannah      0.578725\n",
      "Ingrid      0.576413\n",
      "Laura       0.579165\n",
      "Michael     0.578612\n",
      "Quinn       0.577007\n",
      "Ray         0.576354\n",
      "Sarah       0.578378\n",
      "Wendy       0.577382\n",
      "Yvonne      0.576763\n",
      "Zelda       0.577506\n",
      "Alice       0.577579\n",
      "Charlie     0.577653\n",
      "Frank       0.578527\n",
      "Jerry       0.577240\n",
      "Kevin       0.578577\n",
      "Norbert     0.577230\n",
      "Oliver      0.579696\n",
      "Patricia    0.577015\n",
      "Tim         0.577735\n",
      "Ursula      0.575801\n",
      "Victor      0.578086\n",
      "Xavier      0.576623\n",
      "Name: x, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "df = dask.datasets.timeseries()\n",
    "df2 = df[df.y > 0].groupby(\"name\").x.std()\n",
    "res = df2.compute()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
